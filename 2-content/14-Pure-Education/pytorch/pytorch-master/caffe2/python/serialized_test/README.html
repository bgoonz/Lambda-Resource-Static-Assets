<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
</head>
<body>
<h1 id="serialized-operator-test-framework">Serialized operator test framework</h1>
<p>Major functionality lives in <code>serialized_test_util.py</code></p>
<h2 id="how-to-use">How to use</h2>
<ol type="1">
<li>Extend the test case class from <code>SerializedTestCase</code></li>
<li>Change the <code>@given</code> decorator to <code>@serialized_test_util.given</code>. This runs a seeded hypothesis test instance which will generate outputs if desired in addition to the unseeded hypothesis tests normally run.</li>
<li>[Optional] Add (or change a call of <code>unittest.main()</code> to) <code>testWithArgs</code> in <code>__main__</code>. This allows you to generate outputs using <code>python caffe2/python/operator_test/my_test.py -G</code>.</li>
<li>Run your test <code>python -m pytest caffe2/python/operator_test/my_test.py -G</code> to generate serialized outputs. They will live in <code>caffe2/python/serialized_test/data/operator_test</code>, one zip file per test function. The zip file contains an <code>inout.npz</code> file of the inputs, outputs, and meta data (like device type), a <code>op.pb</code> file of the operator, and <code>grad_#.pb</code> files of the gradients if there are any. Use <code>-O</code> to change the output directory. This also generates a markdown document summarizing the coverage of serialized tests. We can disable generating this coverage document using the <code>-C</code> flag.</li>
<li>Thereafter, runs of the test without the flag will load serialized outputs and gradient operators for comparison against the seeded run. The comparison is done as long as you have a call to assertReferenceChecks. If for any reason the seeded run’s inputs are different (this can happen with different hypothesis versions or different setups), then we’ll run the serialized inputs through the serialized operator to get a runtime output for comparison.</li>
</ol>
<h2 id="coverage-report">Coverage report</h2>
<p><code>SerializedTestCoverage.md</code> contains some statistics about the coverage of serialized tests. It is regenerated every time someone regenerates a serialized test (i.e. running an operator test with the <code>-G</code> option). If you run into merge conflicts for the file, please rebase and regenerate. If you’d like to disable generating this file when generating the serialized test, you can run with <code>-G -C</code>. The logic for generating this file lives in <code>coverage.py</code>.</p>
<p>##Additional Notes</p>
<p>If we’d like to extend the test framework beyond that for operator tests, we can create a new subfolder for them inside <code>caffe2/python/serialized_test/data</code>.</p>
<p>Note, we currently don’t support using other hypothesis decorators on top of <code>given_and_seeded</code>. Hypothesis has some handling to explicitly check that <code>@given</code> is on the bottom of the decorator stack.</p>
<p>If there are multiple calls to assertReferenceChecks in a test function, we’ll serialize and write the last one. The actual input checked may then differ if we refactor a test function that calls this multiple times, though the serialized test should still pass since we then use the serialized input to generate a dynamic output.</p>
</body>
</html>
